{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "# ignoring the future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "dataset = pd.read_csv('.ipynb_checkpoints/Data.csv')\n",
    "\n",
    "    \n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, 3].values\n",
    "\n",
    "'''\n",
    "    - To take care of the missing data we use the imputer object\n",
    "    - strategy='mean' is used because mean values will be stored for missing cells\n",
    "    \n",
    "    - fitting the imputer object to column 1 and 2 values as it contains missing data\n",
    "    \n",
    "    - labelEncoder is used for converting categorical data i.e values in form of words, to numerical value\n",
    "        - Here \"Country\" column is in the form of words\n",
    "        \n",
    "    -   we need to index column number 0, because the countries does not\n",
    "        have any relation with each other, but the labelEncoder\n",
    "        converts them into numbers and create a problem such as\n",
    "        0 < 1 < 2. In this way our model will mis understand data\n",
    "\n",
    "    -   OneHotEncoder does is, it takes a column which has a categorical\n",
    "        data, which has been label encoded, and then splits the column into\n",
    "        multiple columns. The numbers are replaced by 1s and 0s, depending \n",
    "        on which column has what value\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X[:, 1:3] = imputer.fit_transform(X[:, 1:3])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "columntransform = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder=\"passthrough\")\n",
    "X = columntransform.fit_transform(X)\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y = labelencoder_Y.fit_transform(Y)\n",
    "\n",
    "'''\n",
    "    - Splitting the data set into the Training set and Test set\n",
    "      through training set we will predict test data\n",
    "    - test_size = 0.2 means 20 percentage of total data is used for testing, and res\n",
    "        80 percentage is used for training\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "'''\n",
    "    In our above data set, you can see that \n",
    "        - In \"Salary\" and \"Age\" column variables are not on same scale\n",
    "        - \"Age\" values are going from 27 to 50\n",
    "        - \"Salary\" values are going from 48000 to 83000\n",
    "    - This will cause some issues in our machine learning models, because\n",
    "      lot of machine learning models are based on \"Euclidean distance\"\n",
    "      i.e distance = sqrt((x1 - x2) ** 2, (y2 - y1) ** 2)\n",
    "    \n",
    "    - In above data set suppose that \"Age\" column is X and \"Salary\" column is\n",
    "      Y, then Euclidean distance will be dominated by only the Y values\n",
    "      \n",
    "    - There are two ways of feature scaling\n",
    "        - Standardisation\n",
    "            - xstand = (x - mean(x)) / standard_deviation(x)\n",
    "        - Normalisation\n",
    "            - xnorm = (x - min(x)) / (max(x) - min(x))\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.simplefilter(action='ignore', category=DataConversionWarning)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_train[:, 3]\n",
    "X_test = sc_X.transform(X_test) # because X values are already fitted\n",
    "\n",
    "#     - no need to add feature scaling for Y because this is a classification problem\n",
    "#     - If dependent variable depends on a large number of values then we need to add feature scaling to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "\n",
    "dataset = pd.read_csv('.ipynb_checkpoints/Data.csv')\n",
    "\n",
    "    \n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, 3].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.exceptions import DataConversionWarning\n",
    "# warnings.simplefilter(action='ignore', category=DataConversionWarning)\n",
    "# sc_X = StandardScaler()\n",
    "# X_train = sc_X.fit_transform(X_train)\n",
    "# X_train[:, 3]\n",
    "# X_test = sc_X.transform(X_test) # because X values are already fitted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. \n",
    "# If your independent variable is time, then you are forecasting future values, otherwise your model is predicting \n",
    "# present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.\n",
    "\n",
    "# In this part, you will understand and learn how to implement the following Machine Learning Regression models:\n",
    "\n",
    "# Simple Linear Regression\n",
    "# Multiple Linear Regression\n",
    "# Polynomial Regression\n",
    "# Support Vector for Regression (SVR)\n",
    "# Decision Tree Classification\n",
    "# Random Forest Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
